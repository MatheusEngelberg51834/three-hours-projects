I - Linear Regression:

Training Set --> Learning Algorithm --> Hypothesis [H] --> Estimate Value
                   
                                                                               n
Hypothesis ----> h(x) = TETA(0) + TETA(1) . X(1) + ... + TETA(n) . X(n) ----> SUM TETA(j) . X(j), where X(0) = 1
                                                                             j = 0

TETA = [TETA(0),    X = [X(0),
	TETA(1),         X(1),
	  .               .
	  .               .
	  .               .
        TETA(n)]         X(n)]


TETA = parameters
M = number of rows
X = features
Y = target variable
(X, Y) = training example
(X(i), Y(i)) = Ith training example
N = # of features

How to choose parameters (TETA) such that h(x) == y for the training examples?

hTETA(x) = h(x)


                    1    m
minimize TETA -->  ---  SUM  (h(x(i)) - y) ^ 2  --> COST FUNCTION ( J(TETA) )
                    2  i = 1


--- Batch Gradient Descent:

(Not good with big data sets, every step requires a sum of the dataset)

start with some TETA (say TETA n-dimensional vector(0))

keep changing THETA tp reduce J(THETA)

each step is:
	
        REPEAT UNTIL CONVERGENCE:
                              
	THETA(j) assign as  THETA(j) - ALPHA . partial derivative of cost function in respect to the parameter THETA(j) [((h(x) - y) . xj]
------> THETA(j) - ALPHA (learning rate) . ((h(x) - y) . xj ---> THETA(j) - ALPHA . SUM (i = 1 -> m) of the partial derivative 


--- Stochastic Gradient Descent:

loops from i = 1 to m the gradient descent (without sum), the derivative is only in respect to one example

II - Decision Trees:

Greedy, Top-Down, Recursive Partitioning --->

Region Rp, looking for split Sp,
Sp(j,t) = ({X|Xj < t, XERp}, {X|Xj >= t, XERp})

How to choose splits?

Define L(R), loss on R
Given C classes, define ^Pc to be the proportion of examples in R that are of class c --->

Lc = 1 - max ^Pc
	  c
	  
max L(Rp) - (L(R1) + L(R2))
     parent   children loss
     loss
      
Misclassification Loss has issues, instead, define cross-entropy loss

  L     =  - SUM Pc LOG Pc
 cross        c        2

Gini Loss --->

SUM Pc(1 - Pc)
 c
 
Regression Trees --->

  Rm
Predict ^Ym = SUM   (Yi - ^Ym)^2 = Lsquared
	      iERm  ------------
	      		|Rm|

Categorical Vars --->



